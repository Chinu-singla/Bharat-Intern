
#import all libaries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
%matplotlib inline
from sklearn.datasets import load_iris
iris=load_iris()
print(iris.keys())

dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])

print(iris.columns)

df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df

output>
sepal length (cm) 	sepal width (cm) 	petal length (cm) 	petal width (cm)
0 	5.1 	3.5 	1.4 	0.2
1 	4.9 	3.0 	1.4 	0.2
2 	4.7 	3.2 	1.3 	0.2
3 	4.6 	3.1 	1.5 	0.2
4 	5.0 	3.6 	1.4 	0.2
... 	... 	... 	... 	...
145 	6.7 	3.0 	5.2 	2.3
146 	6.3 	2.5 	5.0 	1.9
147 	6.5 	3.0 	5.2 	2.0
148 	6.2 	3.4 	5.4 	2.3
149 	5.9 	3.0 	5.1 	1.8

150 rows Ã— 4 columns

df['Species'] = iris.target_names[iris.target]
df

	sepal length (cm) 	sepal width (cm) 	petal length (cm) 	petal width (cm) 	Species
0 	5.1 	3.5 	1.4 	0.2 	setosa
1 	4.9 	3.0 	1.4 	0.2 	setosa
2 	4.7 	3.2 	1.3 	0.2 	setosa
3 	4.6 	3.1 	1.5 	0.2 	setosa
4 	5.0 	3.6 	1.4 	0.2 	setosa
... 	... 	... 	... 	... 	...
145 	6.7 	3.0 	5.2 	2.3 	virginica
146 	6.3 	2.5 	5.0 	1.9 	virginica
147 	6.5 	3.0 	5.2 	2.0 	virginica
148 	6.2 	3.4 	5.4 	2.3 	virginica
149 	5.9 	3.0 	5.1 	1.8 	virginica

print(iris['data'].shape)
print(iris['target'].shape)

(150, 4)
(150,)

print(iris.target)
#here 0 means setosa,1 means versicolor and 2 means virginica

[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]


from sklearn.model_selection import train_test_split
X_train, X_test,y_train, y_test = train_test_split(iris.data,iris.target,test_size =0.3)
print(X_test)

[[5.7 3.8 1.7 0.3]
 [5.5 2.6 4.4 1.2]
 [5.4 3.9 1.3 0.4]
 [5.  2.3 3.3 1. ]
 [5.8 2.7 4.1 1. ]
 [5.5 4.2 1.4 0.2]
 [6.3 3.4 5.6 2.4]
 [6.3 2.5 4.9 1.5]
 [7.2 3.  5.8 1.6]
 [5.4 3.9 1.7 0.4]
 [4.8 3.4 1.6 0.2]
 [5.4 3.  4.5 1.5]
 [6.4 2.7 5.3 1.9]
 [4.6 3.1 1.5 0.2]
 [6.3 2.9 5.6 1.8]
 [6.2 3.4 5.4 2.3]
 [7.1 3.  5.9 2.1]
 [5.6 2.8 4.9 2. ]
 [4.4 3.  1.3 0.2]
 [6.7 3.3 5.7 2.1]
 [6.1 2.6 5.6 1.4]
 [6.8 3.2 5.9 2.3]
 [6.7 2.5 5.8 1.8]
 [6.6 3.  4.4 1.4]
 [4.7 3.2 1.3 0.2]
 [6.9 3.1 5.4 2.1]
 [6.2 2.8 4.8 1.8]
 [5.5 2.4 3.7 1. ]
 [6.7 3.  5.  1.7]
 [5.1 3.8 1.6 0.2]
 [7.7 2.8 6.7 2. ]
 [5.  3.2 1.2 0.2]
 [5.6 3.  4.1 1.3]
 [6.  2.2 4.  1. ]
 [4.9 3.  1.4 0.2]
 [5.  3.5 1.3 0.3]
 [5.  3.  1.6 0.2]
 [5.5 2.4 3.8 1.1]
 [5.4 3.4 1.7 0.2]
 [6.3 3.3 6.  2.5]
 [6.2 2.2 4.5 1.5]
 [5.1 2.5 3.  1.1]
 [5.7 3.  4.2 1.2]
 [6.  3.4 4.5 1.6]
 [6.4 3.1 5.5 1.8]]

lr = LogisticRegression()
lr.fit(X_train, y_train)
lr.score(X_test, y_test)

0.98555
svm = SVC()
svm.fit(X_train, y_train)
svm.score(X_test, y_test)

0.9777777777777777

rf = RandomForestClassifier(n_estimators=30)
rf.fit(X_train,y_train)
rf.score(X_test,y_test)

0.9453333333333333


